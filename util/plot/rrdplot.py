"""A tool to plot graphs for host and VM OS metric data.

Introduction
------------

The tool saves host and VM OS metric data to RRDtool database and use
RRDtool graph command to plot metrics. It gets metrics and graphs information
from a config file provided by user. Nothing is hardcoded.

The tool defines an abstract interface for getting metric data. It contains
an implemention of that interface based on CSV files generaged by CBTOOL
monextract command. It should be not difficult to add a new implemenation
to read data directly from mongodb.

The tool is designed to be easy to use. Its CLI syntax is simple, the only
required argument is an experiment data directory generated by CBTOOL
monextract command.

Collecting Metrics
------------------

The metric data processed by the tool are collected by CBTOOL. So you should
first set up CBTOOL to enable collecting metrics for host and VMs. Please
refer to this page for details:

https://github.com/ibmcb/cbtool/wiki/DOC:-Metrics-Collection

Once enabled, CBTOOL takes care of Gmetad and Gmond setup in VMs. User need
to set up Gmond on host nodes manually. Below are steps to do that:

1) Install packages

For Ubuntu:

$ sudo apt-get install ganglia-monitor ganglia-monitor-python

For Centos:

$ sudo yum install ganglia-gmond ganglia-gmond-python

2) Install diskstats.py and mem_stats.py modules

These are two Gmond Python modules implemented by CBTOOL.

For Ubuntu:

$ sudo cp $CBTOOL/util/python_ganglia_modules/{diskstats.py,mem_stats.py} \
    /usr/lib/ganglia/python_modules/
$ sudo cp $CBTOOL/util/ganglia_conf.d/{diskstats.conf,mem_stats.conf} \
    /etc/ganglia/conf.d/

For Centos:

$ sudo cp $CBTOOL/util/python_ganglia_modules/{diskstats.py,mem_stats.py} \
    /usr/lib64/ganglia/python_modules/
$ sudo cp $CBTOOL/util/ganglia_conf.d/{diskstats.conf,mem_stats.conf} \
    /etc/ganglia/conf.d/

3) Restart Gmond service to make the change effective

The tool processes both host and VM OS metric data. It's OK if only host
or VM metric data are available (for example, user may not have access to
host nodes, or user doesn't enable VM metric collecting). If both are
unavailable, the tool does nothing.

It's also OK if user specifies a metric to save but it's not available in
the data (for example, suppose user uses the default config file which
includes mem_anonpages metric but user doesn't install mem_stats.py on host
nodes). In that case when the tool saves the metic in RRDtool database, its
value will be 'U' (meaning: unknown value) and RRD will ignore it when
plotting graphs.

How to Run It
-------------

First run CBTOOL monextract command to save experiment metric data in CSV
files. The command creates a directory for that experiment and generate CSV
files for host OS metrics, VM OS metrics, VM provision metrics, and VM
application metrics, repsectively.

Then run the tool as below:

  $ python rrdplot.py <experiment_dir>

That's it. For more information about its options, please run it with -h.

Config File
-----------

The tool needs a config file, which defines what metrics to save into RRDtool
database and what graphs to plot. A default config file is shipped with the
tool and is available as rrdtool.yml under the same directory where the tool
is installed. That file contains detailed description about config file format,
as well as how to define metrics and graphs.

The tool uses the shipped config file by default. User can change config file
location with -c option.

Output Files
------------

The tool first creates a RRDtool database file for each host and VM:

  HOST_runtime_os_<expid>_host_<hostname>.rrd
  VM_runtime_os_<expid>_vm_<vmid>.rrd

Then it creates all graphs defined by user for each host and VM, and combines
them into a single image. For example, the default config file defines four
graphs: cpu, memory, diskio, netioi, so the tool generates five files for
each host:

  HOST_runtime_os_<expid>_host_<hostname>_{cpu,memory,diskio,netio}.png
  HOST_runtime_os_<expid>_host_<hostname>.png (combining the above graphs)

and five files for each VM:

  VM_runtime_os_<expid>_vm_<vmid>_{cpu,memory,diskio,netio}.png
  VM_runtime_os_<expid>_vm_<vmdid>.png (combining the above graphs)

Finally it combines all hosts and VMs graphs for each graph type. For example,
with default config file the tool generates four combined files:

  HOST_runtime_os_<expid>_{cpu,memory,diskio,netio}.png

rrdtool_interval Option
-----------------------

The tool supports a rrdtool_interval option, which specifies the interval
in seconds with which metric data are saved in RRDtool database. It defaults
to 60 seconds. It's unlikely user will need to change it. Below are my
explanation why I introduced it. It's safe to skip it if you're not interested.

Gmetad reports data every 15 secs by default. However, many metrics are
sampled in an interval longer than that (see their gmond config files). This
may cause gmetad reports inaccurate data, which is particularly problematic
for metrics that are incrementing counters (e.g., metrics reproted by
diskstats.py module). The issue can be resolved by using rrdtool's
consolidation feature. We'll set up rrdtool to accept data in the same
interval as gmetad generates, but consolidate multiple data into a single
one and only save that one in database. The result is that the data saved
in rrdtool has a longer interval. The rrdtool_interval parameter defines
that interval value in seconds. It's optional and defaults to 60 seconds,
which should be sufficient in most cases.

Future Work
-----------

I wrote the tool to visualize host and VM OS metrics to understand how
resources are used in them. I didn't consider VM application metrics, which
is also time series data. It might be implemented in future.
"""

import argparse
from collections import OrderedDict
import datetime
import logging
from os.path import basename, dirname, isfile, realpath, splitext
import sys

from dateutil.parser import parse as parsedate
from dateutil.tz import tzutc
import jsonschema
from PIL import Image
import rrdtool
import yaml


class Config:
    """Load and validate the tool's config file."""

    SCHEMA = {
        "type": "object",
        "properties": {
            "unit_groups": {
                "type": "object",
                "patternProperties": {
                    ".{1,}": {
                        "type": "object",
                        "patternProperties": {
                            ".{1,}": {"type": "number"}
                        }
                    }
                }
            },
            "metrics": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "type": {
                            "type": "string",
                            "enum": ["GAUGE",
                                     "COUNTER",
                                     "DCOUNTER",
                                     "DERIVE",
                                     "DDERIVE",
                                     "ABSOLUTE"
                            ]
                        },
                        "unit": {"type": "string"}
                    },
                    "required": ["name", "type"]
                }
            },
            "colors": {
                "type": "object",
                "patternProperties": {
                    ".{1,}": {
                        "type": "string",
                        "pattern": "^#[0-9a-f]{6}$"
                    }
                }
            },
            "graphs": {
                "type": "object",
                "patternProperties": {
                    ".{1,}": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "y-axis": {
                                "type": "object",
                                "properties": {
                                    "label": {"type": "string"},
                                    "min_value": {"type": "number"},
                                    "max_value": {"type": "number"},
                                    "rigid": {"type": "boolean"}
                                },
                            },
                            "stack": {"type": "boolean"},
                            "metrics": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "name": {"type": "string"},
                                        "name_in_graph": {"type": "string"},
                                        "unit_in_graph": {"type": "string"},
                                        "color": {"type": "string"},
                                        "transparency": {"type": "number"},
                                    },
                                    "required": ["name", "color"]
                                }
                            }
                        },
                        "required": ["metrics"]
                    }
                }
            }
        }
    }

    def __init__(self, config_file):
        try:
            logging.info("Loading config file: %s" % config_file)
            with open(args.config_file) as f:
                self.config = yaml.load(f)
        except Exception as e:
            logging.exception("Failed to load config file: %s. "
                              "See stack trace below:" % config_file)
            sys.exit(1)

        try:
            logging.info("Validating it")
            jsonschema.validate(self.config, self.SCHEMA)
        except jsonschema.exceptions.ValidationError as e:
            logging.error("Config file has invalid syntax: %s. " %
                          e.message)
            path = [i if not isinstance(i, int) else 'item #%d' % i
                    for i in list(e.path)]
            path = "->".join(path)
            logging.error("The error occurred while parsing: %s" % path)
            sys.exit(1)

    def __getitem__(self, key):
        return self.config[key]


class UnitGroup:

    def __init__(self, name):
        self.name = name
        self.units = {}

    def add(self, unit_name, unit_value):
        self.units[unit_name] = unit_value

    def __contains__(self, item):
        return item in self.units

    def get_conversion_rate(self, old_unit, new_unit):
        """Return a rate to convert the metric data to new unit, as below.

            value in old unit / rate = value in new unit
        """

        for i in [old_unit, new_unit]:
            if i not in self.units:
                raise Exception("Can't find unit %s in unitgroup '%s'" %
                                (i, self.name))
        return float(self.units[new_unit]) / float(self.units[old_unit])


class Metric:
    """Represent a metric whose data will be imported to rrdtool."""

    def __init__(self, name, type, unitgroup=None, unit=None):
        self.name = name
        self.type = type
        self.unitgroup = unitgroup
        self.unit = unit

    def get_conversion_rate(self, newunit):
        """Return a rate to convert the metric data to new unit.

        The rate is used by rddtool graph command when it plots data in
        unit different from the one in which data is stored, as below:

            value in old unit / rate = value in new unit
        """
        if not self.unit or not self.unitgroup:
            logging.error("Metric %s can't be converted into %s unit. "
                          "Please correct your config file." % (self.name,
                                                                newunit))
            sys.exit(1)

        try:
            return self.unitgroup.get_conversion_rate(self.unit, newunit)
        except Exception as e:
            logging.error("Failed to convert metric %s into unit %s. "
                          "Reason: %s. Please correct your config file."
                          % (self.name, newunit, e.message))
            sys.exit(1)


class GraphInfo:
    """Represent a graph to be plotted."""

    def __init__(self, name, title, metrics, stack=False,
                 y_axis_label=None, y_axis_min_value=None,
                 y_axis_max_value=None, y_axis_rigid=None):
        self.name = name
        self.title = title
        self.metrics = metrics
        self.stack = stack
        self.y_axis_label = y_axis_label
        self.y_axis_min_value = y_axis_min_value
        self.y_axis_max_value = y_axis_max_value
        self.y_axis_rigid = y_axis_rigid


class DataSource:
    """An abstract class defining the interface to access metric data.

    For example, its subclasses may read metric data from csvfile or mongodb.
    """

    def get_host_data(self):
        """Return host metric data in a dictionary.

        The dictionary contains one item for each host. Item"s key is host
        name, value is a list of data for that host.
        """

        raise NotImplementedError

    def get_vm_data(self):
        """Return VM metric data in a dictionary.

        The dictionary contains one item for each VM. Item"s key is VM name,
        value is a list of data for that VM.
        """

        raise NotImplementedError

    def get_hosts(self):
        """Return host names in a list."""

        raise NotImplementedError

    def get_vms(self):
        """Return VM names in a list."""

        raise NotImplementedError

    def get_host_data_fields(self):
        """Return the fields of a host data record in a list."""

        raise NotImplementedError

    def get_vm_data_fields(self):
        """Return the fields of a VM data record in a list."""

        raise NotImplementedError

    def get_time_info(self):
        """Return data time information in a tuple.

        Tuple format is:

        (start time, end time, step)

        Start time and end time define a time frame within which all
        data were generated. Step is the average interval in which data
        were generated."""

        raise NotImplementedError

    def get_host_outfiles(self):
        """Return host output files in a tuple.

        Tuple format is:

        (directory, file name prefix, a dictionary containing output files)

        Directory is where the output files will be created.

        File name prefix is the common prefix for all files.

        The dictionary contains one item for each host. Item"s key is host
        name, value is a list of generated file names for this host. These
        files contain rrdtool database file and graph files.
        """

        raise NotImplementedError

    def get_vm_outfiles(self):
        """Return VM output files in a tuple.

        Tuple format is:

        (directory, file name prefix, a dictionary containing output files)

        Directory is where the output files will be created.

        File name prefix is the common prefix for all files.

        The dictionary contains one item for each VM. Item"s key is host
        name, value is a list of generated file names for this VM. These
        files contain rrdtool database file and graph files.
        """

        raise NotImplementedError

    def get_allinone_outfiles(self):
        """Return output files containing combined graphs.

        Tuple format is:

        (directory, file name prefix, a dictionary containing output files)

        Directory is where the output files will be created.

        File name prefix is the common prefix for all files.

        The dictionary contains three items, each of which is a dictionary
        and contains output files for combined graphs of host, VM and metric,
        respectively.
        """

        raise NotImplementedError


class CSVFile(DataSource):
    """A subclass of DataSource that reads metric data from CSV files.

    The CSV files are generated by CBTOOL monextract command.
    """

    REQUIRED_FIELDS = ["time", "time_h", "name"]
    HOST_FILE_PREFIX = "HOST_runtime_os"
    VM_FILE_PREFIX = "VM_runtime_os"

    def __init__(self, expdir):
        """Initialize a CSVFile object with a directory.

        The directory is CBTOOL experiment result directory generated by
        monextract command. It contains a few CSV files. Among those files,
        one contains host OS metric data, and another VM OS metric data.
        """

        self.expdir = expdir
        self.expid = basename(expdir)
        self.host_csvfile = "%s/%s_%s.csv" % (expdir,
                                              self.HOST_FILE_PREFIX, self.expid)
        self.host_data = {}
        self.host_data_fields = []
        self.host_outfiles = {}
        self.vm_csvfile = "%s/%s_%s.csv" % (expdir,
                                            self.VM_FILE_PREFIX, self.expid)
        self.vm_data = {}
        self.vm_data_fields = []
        self.vm_outfiles = {}
        self.allinone_outfiles = {}
        self.start = None
        self.end = None
        self.step = None

        # Check if the CSV files exist
        for f in [self.host_csvfile, self.vm_csvfile]:
            if not isfile(f):
                logging.error("%s doesn't exist. Aborted." % f)
                sys.exit(1)

        logging.info("Processing files in %s directory" % basename(expdir))
        self.get_host_data()
        self.get_vm_data()
        self.get_time_info()

    def get_host_data(self):
        """Implement DataSource.get_host_data() method."""

        if not self.host_data:
            self.host_data, self.host_data_fields = self.parse_csvfile(
                self.host_csvfile)
        return self.host_data

    def get_vm_data(self):
        """Implement DataSource.get_vm_data() method."""

        if not self.vm_data:
            self.vm_data, self.vm_data_fields = self.parse_csvfile(
                self.vm_csvfile)
        return self.vm_data

    def get_hosts(self):
        """Implement DataSource.get_hosts() method.

        The returned host names are sorted in alphabetic order."""

        return sorted(self.host_data.keys())

    def get_vms(self):
        """Implement DataSource.get_vms() method.

        VM names in CSV files are of "vm_<id>" format. The returned VM names
        are sorted in integer id order."""

        vms = [v for v in self.vm_data.keys()]
        vms.sort(lambda x, y: cmp(int(x[3:]), int(y[3:])))
        return vms

    def get_host_data_fields(self):
        """Implement DataSource.get_host_data_fields() method."""

        return self.host_data_fields

    def get_vm_data_fields(self):
        """Implement DataSource.get_vm_data_fields() method."""

        return self.vm_data_fields

    def parse_csvfile(self, csvfile):
        """Read metric data from a CSV file.

        The method returns a tuple:

        (metric_data, fields)

        metric_data is a dictionary. It has one item for each node (a node is
        a host or VM). Item"s key is node name, value is a list containing that
        node"s data.

        Fields is a list containing all fields of a node"s data record.
        """

        logging.info("Parseing csvfile: %s" % basename(csvfile))
        fields = []
        data = {}
        try:
            with open(csvfile) as f:
                for line in f:
                    line = line.strip()
                    # Skip empty or commented line
                    if not line or line[0] == "#":
                        continue
                    if not fields:
                        # The first valid line defines fields.
                        fields = [x.strip() for x in line.split(",")]
                        for f in self.REQUIRED_FIELDS:
                            if f not in fields:
                                logging.error("Failed to find %s field. "
                                              "Aborted." % f)
                                sys.exit(1)
                    else:
                        # The rest lines are data
                        values = [x.strip() for x in line.split(",")]
                        record = {}
                        for k, v in zip(fields, values):
                            record[k] = v
                        # Convert date time string to epoch seconds
                        record["time_h"] = self.parse_timestr(record["time_h"])
                        node = record["name"]
                        if data.get(node, None):
                            data[node].append(record)
                        else:
                            data[node] = [record]
        except Exception as e:
            logging.exception("Failed to parsing the csvfile. "
                              "See stack trace below:")
            sys.exit(1)

        # While it didn't occur often, I observed that data in CSV files
        # generated by cbtool monextrac command were not in time order.
        # So sort them.
        logging.debug("Sorting the data")
        for node in data.keys():
            data[node].sort(lambda x, y: cmp(int(x["time"]), int(y["time"])))

        return data, fields

    def get_time_info(self):
        """Implement DataSource.get_time_info() method."""

        if self.start and self.end and self.step:
            return (self.start, self.end, self.step)

        host_time_info = self._get_time_info(self.host_data,
                                             self.get_hosts(),
                                             "host")
        if host_time_info:
            self.start, self.end, self.step = host_time_info
            return host_time_info

        # If it reaches here, host OS metrics file must contain no data, so
        # get time information from VM OS metrics file.
        vm_time_info = self._get_time_info(self.vm_data,
                                           self.get_vms(),
                                           "VM")
        if vm_time_info:
            self.start, self.end, self.step = vm_time_info
            return vm_time_info

        # If it reaches here, VM OS metrics file contains no data too. Abort.
        logging.error("Found no data in both host and VM OS metric files. "
                      "Aborted.")
        sys.exit(1)

    def _get_time_info(self, data, nodes, nodetype):
        logging.info("Getting time information from %s data" % nodetype)

        time_info_list = []
        for node in nodes:
            nodedata = data[node]
            # Skip it if it doesn't have enough data
            if len(nodedata) < 2:
                continue
            # The data has been sorted, so just use the first and last one.
            start = nodedata[0]["time_h"]
            end = nodedata[-1]["time_h"]
            duration = end - start
            step = float(duration) / len(nodedata)
            time_info_list.append((start, end, step))

        if time_info_list:
            start, end, step = reduce(lambda x, y:
                                      (x[0] if x[0] < y[0] else y[0],  # start
                                       x[1] if x[1] > y[1] else y[1],  # end
                                       (x[2] + y[2]) / 2),             # step
                                      time_info_list)
            step = int(round(step))

            # Sanity check
            if start == 0 or end == 0 or step == 0:
                logging.error("Failed to get time information from %s data: "
                              "start time : %d, end time: %d, step: %d. "
                              "Aborted." % (start, end, step))
                sys.exit(1)

            logging.info("start time : %d, end time: %d, step: %d. " %
                         (start, end, step))
            return (start, end, step)
        else:
            logging.warn("%s OS metric data file contain no data" % nodetype)
            return None

    def parse_timestr(self, timestr):
        """Parse a datatime string and return seconds since epoch."""

        epoch = datetime.datetime(1970, 1, 1, 0, 0, 0, 0, tzutc())
        return int((parsedate(timestr) - epoch).total_seconds())

    def get_host_outfiles(self):
        """Implement DataSource.get_host_outfiles() method."""

        if not self.host_outfiles:
            # Initialize it
            for node in self.get_hosts():
                self.host_outfiles[node] = OrderedDict()

        return (self.expdir,
                "%s_%s" % (self.HOST_FILE_PREFIX, self.expid),
                self.host_outfiles)

    def get_vm_outfiles(self):
        """Implement DataSource.get_vm_outfiles() method."""

        if not self.vm_outfiles:
            # Initialize it
            for node in self.get_vms():
                self.vm_outfiles[node] = OrderedDict()

        return (self.expdir,
                "%s_%s" % (self.VM_FILE_PREFIX, self.expid),
                self.vm_outfiles)

    def get_allinone_outfiles(self):
        """Implement DataSource.get_allinone_outfiles() method."""

        if not self.allinone_outfiles:
            # Initialize it
            self.allinone_outfiles["hosts"] = OrderedDict()
            self.allinone_outfiles["vms"] = OrderedDict()
            self.allinone_outfiles["metrics"] = OrderedDict()

        return self.expdir, "%s" % (self.expid), self.allinone_outfiles


class RRDToolDB:
    """Represent a RRDTool database which user can issue commands on."""

    SKIPPED = "(skipped)"

    def __init__(self, node, type, ds, mr, rrdtool_step):
        """Initialize a RRDToolDB object.

        node: the node for which the database contains data.
        type: the node type. Valid values are "host" and "VM"
        ds: DataSource object
        mr: MetricRegistry object
        rrdtool_step: the interval in which metric data are saved in
        rrdtool database.

        The method creates a RRDTool database for the node, using the
        start/end time returned by ds object. Then it retrieves the node"s
        data from ds object and saves the value of the metrics specified
        by mr object, as well as their timestamps, in the database.
        """

        self.node = node
        if type == "host":
            self.data = ds.get_host_data()[node]
            self.topdir, self.file_prefix, outfiles = ds.get_host_outfiles()
            self.outfiles = outfiles[self.node]
        elif type == "vm":
            self.data = ds.get_vm_data()[node]
            self.topdir, self.file_prefix, outfiles = ds.get_vm_outfiles()
            self.outfiles = outfiles[self.node]
        else:
            raise Exception("Invalide type value: %s" % type)
        self.start, self.end, self.datasource_step = ds.get_time_info()
        self.rrdtool_step = rrdtool_step
        self.conrate = self.rrdtool_step / self.datasource_step

        if len(self.data)/self.conrate < 1:
            logging.warn("%s doesn't have enough data. Skipped it." % node)
            self.outfiles["rrdfile"] = self.SKIPPED
            self.rrdfile = None
            return

        self.outfiles["rrdfile"] = "%s/%s_%s.rrd" % (self.topdir,
                                                     self.file_prefix, node)
        self.rrdfile = self.outfiles["rrdfile"]

        # Create db
        logging.info("Creating rrdtool database for %s" % node)
        dslist = ["DS:%s:%s:%d:0:U" % (mobj.name, mobj.type,
                                       2*self.datasource_step)
                  for mname, mobj in mr.items()]
        self.rrdtool_cmd("create", self.outfiles["rrdfile"],
                         dslist,
                         "--start", str(self.start - 1),
                         "--step", str(self.datasource_step),
                         "RRA:AVERAGE:0.5:%d:%d" %
                                 (self.conrate, len(self.data)/self.conrate),
                         log_level=logging.DEBUG)

        # Import data
        logging.info("Importing data to the database")
        fields = [mname for mname, mobj in mr.items()]
        for d in self.data:
            template = ":".join(fields)
            # If the data doesn't have a field for this metric or if it has
            # but the value of the field is "NA", change it to "U" (this means
            # unknown value in RRDTool).
            values = ":".join([d.get(k, "U").replace("NA", "U")
                               for k in fields])
            self.rrdtool_cmd("update", self.outfiles["rrdfile"],
                             "--template", template,
                             "%s:%s" % (d["time_h"], values))

    def plot_graph(self, graphinfo):
        """Plot graph using information in graphinfo object."""

        WIDTH = 450
        HEIGHT = WIDTH * 0.55
        opts = []

        # Generate outfile name
        if not self.rrdfile:
            self.outfiles[graphinfo.name] = self.SKIPPED
            return

        logging.info("Plotting %s graph for %s" % (graphinfo.name, self.node))
        self.outfiles[graphinfo.name] = "%s/%s_%s_%s.png" % (self.topdir,
                                                             self.file_prefix,
                                                             self.node,
                                                             graphinfo.name)
        opts = opts + [self.outfiles[graphinfo.name]]

        # Generate general image options
        opts = opts + ["--width", str(WIDTH),
                       "--height", str(HEIGHT),
                       "--slope-mode"]

        # Generate title
        if graphinfo.title:
            opts = opts + ["--title", "%s (%s)" % (graphinfo.title, node)]

        # Generate X-axis options
        start, end, step = ds.get_time_info()
        duration = end - start
        mg_step = duration / 10
        bg_step = mg_step / 5
        label_step = mg_step
        if mg_step == 0 or bg_step == 0:
            # This is unlikely to happen, but just to be on the safe side.
            x_grid = "SECOND:1:SECOND:10:SECOND:10:0:%R"
        else:
            x_grid = "SECOND:%s:SECOND:%s:SECOND:%s:0:%%R" % \
                     (bg_step, mg_step, label_step)
        opts = opts + ["--start", str(self.start),
                       "--end", str(self.end),
                       "--step", str(self.rrdtool_step),
                       "--x-grid", x_grid]

        # Generate Y-axis options
        if graphinfo.y_axis_label:
            opts = opts + ["--vertical-label", graphinfo.y_axis_label]
        if graphinfo.y_axis_min_value == 0 or graphinfo.y_axis_min_value:
            opts = opts + ["--lower-limit", str(graphinfo.y_axis_min_value)]
        if graphinfo.y_axis_max_value == 0 or graphinfo.y_axis_max_value:
            opts = opts + ["--upper-limit", str(graphinfo.y_axis_max_value)]
        if graphinfo.y_axis_rigid:
            opts = opts + ["--rigid"]

        # Generate metric parameters
        stack_opt = ""
        if graphinfo.stack:
            stack_opt = ":STACK"
        deflist = []
        cdeflist = []
        arealist = []
        for i in graphinfo.metrics:
            name, name_in_graph, unit_in_graph, color = i
            if unit_in_graph:
                new_unit, rate = unit_in_graph
                newname = "%s_%s" % (name, new_unit)
                deflist.append("DEF:%s=%s:%s:AVERAGE" %
                               (name, self.rrdfile, name))
                cdeflist.append("CDEF:%s=%s,%s,/" %
                                (newname, name, rate))
                arealist.append("AREA:%s%s:%s%s" %
                                (newname, color, name_in_graph, stack_opt))
            else:
                deflist.append("DEF:%s=%s:%s:AVERAGE" %
                               (name, self.rrdfile, name))
                arealist.append("AREA:%s%s:%s%s" %
                                (name, color, name_in_graph, stack_opt))
        opts = opts + deflist + cdeflist + arealist

        self.rrdtool_cmd("graph", opts, log_level=logging.DEBUG)

    def rrdtool_cmd(self, cmd, *args, **kwargs):
        """A wrapper of rrdtool functions, with additional logging function."""

        fn_table = {"create": rrdtool.create,
                    "update": rrdtool.update,
                    "graph": rrdtool.graph}
        fn = fn_table[cmd]
        cmdline = "rrdtool %s %s" % (cmd,
            " ".join([i if isinstance(i, str) else " ".join(i) for i in args]))
        log_level = kwargs.get("log_level", None)
        if log_level:
            # rrdtool command arguments are either string or list of strings.
            logging.log(log_level, cmdline)
        try:
            fn(*args)
        except Exception as e:
            logging.exception("RRDTool command failed. See stack trace below:")
            logging.error("Failed command: %s" % cmdline)
            sys.exit(1)


class GraphUtil:
    """Provide graphic utility methods."""

    @classmethod
    def combine_graphs(cls, ds, gr):
        """Combine graphs for each host, each VM, and each type of graph.

        The function gets generated graphs from ds object, and combine
        them for each host, each VM, and each type of graph.
        """

        topdir, file_prefix, outfiles = ds.get_allinone_outfiles()

        # For each host, combine all its graphs
        _, _, host_outfiles = ds.get_host_outfiles()
        for node in ds.get_hosts():
            logging.info("Combining graphs for %s" % node)
            graphs = [v for k, v in host_outfiles[node].items()
                      if k != "rrdfile" and v != RRDToolDB.SKIPPED]
            if graphs:
                newgraph = "%s/%s_%s.png" % (topdir, file_prefix, node)
                cls.combine_graphs_vertically(graphs, newgraph)
                outfiles["hosts"][node] = newgraph
            else:
                outfiles["hosts"][node] = RRDToolDB.SKIPPED

        # For each VM, combine all its graphs
        _, _, vm_outfiles = ds.get_vm_outfiles()
        for node in ds.get_vms():
            logging.info("Combining graphs for %s" % node)
            graphs = [v for k, v in vm_outfiles[node].items()
                      if k != "rrdfile" and v != RRDToolDB.SKIPPED]
            if graphs:
                newgraph = "%s/%s_%s.png" % (topdir, file_prefix, node)
                cls.combine_graphs_vertically(graphs, newgraph)
                outfiles["vms"][node] = newgraph
            else:
                outfiles["vms"][node] = RRDToolDB.SKIPPED

        # For each type of graphs (e.g., cpu, memory, etc.), combine all
        # graphs from hosts and VMs
        for gname in gr:
            logging.info("Combining all host and VM graphs for %s" % gname)
            host_graphs = [host_outfiles[node][gname] for node in ds.get_hosts()
                           if host_outfiles[node][gname] != RRDToolDB.SKIPPED]
            vm_graphs = [vm_outfiles[node][gname] for node in ds.get_vms()
                         if vm_outfiles[node][gname] != RRDToolDB.SKIPPED]
            if host_graphs + vm_graphs:
                newgraph = "%s/%s_%s.png" % (topdir, file_prefix, gname)
                cls.combine_graphs_vertically(host_graphs + vm_graphs, newgraph)
                outfiles["metrics"][gname] = newgraph
            else:
                outfiles["metrics"][gname] = RRDToolDB.SKIPPED

    @classmethod
    def combine_graphs_vertically(cls, graphs, newgraph):
        """Combine multiple images vertically.

        graphs: the image files to be combined
        newgraph: the output file.
        """

        # Calculate width and height for the new graph
        imgs = [Image.open(f) for f in graphs]
        width, height = (0, 0)
        for i in imgs:
            w, h = getattr(i, "size")
            if w > width:
                width = w
            height = height + h

        newimg = Image.new("RGB", (width, height))
        y = 0
        for i in imgs:
            w, h = getattr(i, "size")
            newimg.paste(i, (0, y))
            y = y + h

        newimg.save(newgraph)

    @classmethod
    def alpha(cls, rgb_color, transparency):
        """Set transparency to a color

        The function returns a new color code in #RRGGBBAA format.

        rgb_color: original color code in #RRGGBB format (no alpha channel).

        transparency: alpha value. Its range is 0 to 1. 0 means full
        transparency. 1 means full opacity.
        """

        if transparency > 1:
            transparency = 1
        elif transparency < 0:
            transparency = 0
        return rgb_color + str(hex(int(254 * transparency)))[2:]

def parse_docstring(text):
    # Get first line
    n = text.find("\n")
    title = text[:n]

    # Skip a blank line
    n = text.find("\n", n + 1)
    body = text[n+1:]

    return (title, body)

def setup_logger(logfile):
    """"""
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    fh = logging.FileHandler(logfile, mode="w")
    fh.setLevel(logging.DEBUG)

    formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)

    print "More detailed logging messages are saved in:\n  %s\n" % logfile


def initialize(config_file):
    """Load config file and instantiate UnitGroup, Metric and Graph objects."""

    config = Config(config_file)

    # Instantiate UnitGroup objects, based on definition in config file.
    ugregistry = {}
    for ugname, ugcfg in config["unit_groups"].items():
        ugobj = UnitGroup(ugname)
        for unit, value in ugcfg.items():
            ugobj.add(unit, value)
        ugregistry[ugname] = ugobj

    # Instantiate Metric objects, based on definition in config file.
    mregistry = {}
    for mcfg in config["metrics"]:
        # Metric unit attribute is optional.
        unit = mcfg.get("unit", None)
        unitgroup = None
        if unit:
            for ugname, ugobj in ugregistry.items():
                if unit in ugobj:
                    unitgroup = ugobj
                    break
            if not unitgroup:
                logging.error("Failed to find unit group for metric %s's unit: "
                              "%s. Please correct your config file." %
                              (mcfg["name"], unit))
                sys.exit(1)
        mobj = Metric(mcfg["name"], mcfg["type"], unitgroup, unit)
        mregistry[mcfg["name"]] = mobj

    # Save color values, based on definition in config file.
    cregistry = {}
    for cname, cvalue in config["colors"].items():
        cregistry[cname] = cvalue

    # Instantiate GraphInfo objects, based on definition in config file.
    gregistry = {}
    for gname, gcfg in config["graphs"].items():
        gcfg["name"] = gname
        metrics = []
        for mcfg in gcfg["metrics"]:
            # Only name and color are required in schema. Others are optional.
            try:
                mobj = mregistry[mcfg["name"]]
            except Exception:
                logging.error("Failed to find metric %s for graph %s. "
                              "Please correct your config file." %
                              (mcfg["name"], gname))
                sys.exit(1)
            name_in_graph = mcfg.get("name_in_graph", mobj.name)
            unit_in_graph = mcfg.get("unit_in_graph", None)
            if unit_in_graph and unit_in_graph != mobj.unit:
                rate = mobj.get_conversion_rate(unit_in_graph)
                unit_in_graph = (unit_in_graph, rate)
            else:
                unit_in_graph = None
            color = cregistry[mcfg.get("color")]
            transparency = mcfg.get("transparency", 1)
            if transparency != 1:
                color = GraphUtil.alpha(color, transparency)
            metrics.append((mcfg["name"],
                            name_in_graph,
                            unit_in_graph,  # Tuple
                            color))

        # All attributes except metrics are optional. Set their default
        # value if not specified.
        title = gcfg.get("title", None)
        stack = gcfg.get("stack", False)
        y_axis = gcfg.get("y-axis", None)
        if y_axis:
            y_axis_label = y_axis.get("label", None)
            y_axis_min_value = y_axis.get("min_value", None)
            y_axis_max_value = y_axis.get("max_value", None)
            y_axis_rigid = y_axis.get("rigid", None)
        else:
            y_axis_label = None
            y_axis_min_value = None
            y_axis_max_value = None
            y_axis_rigid = None

        gobj = GraphInfo(gname,
                         title,
                         metrics,
                         stack,
                         y_axis_label,
                         y_axis_min_value,
                         y_axis_max_value,
                         y_axis_rigid)
        gregistry[gname] = gobj

    return mregistry, gregistry


def print_result(ds):
    print "\nHost Graphs"
    print "-----------"
    _, _, host_outfiles = ds.get_host_outfiles()
    for node in ds.get_hosts():
        print "\n  - node: %s" % node
        for k, v in host_outfiles[node].items():
            print "  - %s: %s" % (k, basename(v))

    print "\nVM Graphs"
    print "---------"
    _, _, vm_outfiles = ds.get_vm_outfiles()
    for node in ds.get_vms():
        print "\n  - node: %s" % node
        for k, v in vm_outfiles[node].items():
            print "  - %s: %s" % (k, basename(v))

    print "\nAll-in-One Graphs"
    print "-----------------"
    _, _, allinone_outfiles = ds.get_allinone_outfiles()
    for k, v in allinone_outfiles["hosts"].items():
        print "  - %s: %s" % (k, basename(v))

    print ""
    for k, v in allinone_outfiles["vms"].items():
        print "  - %s: %s" % (k, basename(v))

    print ""
    for k, v in allinone_outfiles["metrics"].items():
        print "  - %s: %s" % (k, basename(v))


if __name__ == "__main__":
    program_fullpath = realpath(sys.argv[0])
    program_dir = dirname(program_fullpath)
    program_name = splitext(basename(program_fullpath))[0]

    parser = argparse.ArgumentParser()
    parser.add_argument("experiment_dir",
                        help=("Experiment data directory. It contains the "
                              "csv files to be processed. The directory and "
                              "its files are generated by CBTOOL monextract "
                              "command"))
    parser.add_argument("-c", dest='config_file',
                        default="%s/%s.yml" % (program_dir, program_name),
                        help=("The config file that defines metrics and graphs "
                              "to plot."))
    parser.add_argument("-i", dest='rrdtool_interval', type=int, default=60,
                        help=("The interval in seconds with which metric data "
                              "are saved in rrdtool database."))
    args = parser.parse_args()
    setup_logger("%s/%s.log" % (args.experiment_dir, program_name))
    mr, gr = initialize(args.config_file)
    ds = CSVFile(args.experiment_dir)

    # Create rrdtool databases for hosts
    for node in ds.get_hosts():
        db = RRDToolDB(node, "host", ds, mr, args.rrdtool_interval)
        for gname, gobj in gr.items():
            db.plot_graph(gobj)

    # Create rrdtool databases for VMs
    for node in ds.get_vms():
        db = RRDToolDB(node, "vm", ds, mr, args.rrdtool_interval)
        for gname, gobj in gr.items():
            db.plot_graph(gobj)

    GraphUtil.combine_graphs(ds, gr)
    print_result(ds)
